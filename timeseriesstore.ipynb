{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from zfstools import connection\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastparquet\n",
    "import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2G\t/mnt/sdb/forex\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "du -sh /mnt/sdb/forex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "zfs list -t snapshot -o name | tail -n +2 | xargs -I {} zfs destroy {}\n",
    "rm /tss/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_test():\n",
    "    paths = [os.path.join('/tss/.zfs/snapshot/1', f) for f in os.listdir('/tss/.zfs/snapshot/1')]\n",
    "    \n",
    "    def full_series():\n",
    "        for p in tqdm.tqdm(paths):\n",
    "            df = pq.read_table(p).to_pandas()\n",
    "    \n",
    "    def last_value():\n",
    "        for p in tqdm.tqdm(paths):\n",
    "            pq_file = pq.ParquetFile(p)\n",
    "            last = pq_file.read_row_group(pq_file.num_row_groups - 1).to_pandas()[-1:]\n",
    "            \n",
    "    def all_values_one_day(day):\n",
    "        for p in tqdm.tqdm(paths):\n",
    "            df = pq.read_table(p).to_pandas()\n",
    "            df_one_day = df[df.seriesdate.str.startswith(day)]\n",
    "            \n",
    "    full_series()\n",
    "    last_value()\n",
    "    all_values_one_day('20090701')\n",
    "    \n",
    "# Splits a df into n+1 parts where the first part consist of the first tot-n rows\n",
    "def split_df(df, n):\n",
    "    return [df[:-n]] + [df[n+i:n+i+1] for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forex_loc = '/mnt/sdb/forex'\n",
    "paths = [os.path.join(forex_loc, f) for f in os.listdir(forex_loc)]\n",
    "zfs = connection.ZFSConnection()\n",
    "names = ['symbol', 'seriesdate', 'low', 'high']\n",
    "\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50541046"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [df for p in paths for df in np.array_split(pd.read_csv(p, names=names), 400)]\n",
    "for i, df in zip(range(len(dfs)), dfs): df.symbol = i\n",
    "valueses = map(lambda x: split_df(x, n), dfs)\n",
    "sum(map(lambda x: len(x), dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TimeSeriesStoreSingleThread(object):\n",
    "    def __init__(self):\n",
    "        self.max_row_groups = 50\n",
    "\n",
    "    def write(self, dfs, pid):\n",
    "        for df_i, df in tqdm.tqdm(zip(range(len(dfs)), dfs)):\n",
    "            fastparquet.write('/tss/{}.parquet'.format(df_i), df, file_scheme='simple')\n",
    "        zfs.snapshot_recursively('tss', pid)\n",
    "    \n",
    "    def append(self, dfs, pid):\n",
    "        for df_i, df in zip(range(len(dfs)), dfs):\n",
    "            path = '/tss/{}.parquet'.format(df_i)\n",
    "            if len(fastparquet.api.ParquetFile(path).row_groups) > self.max_row_groups:\n",
    "                fastparquet.write(path, fastparquet.api.ParquetFile(path).to_pandas(), file_scheme='simple')\n",
    "            fastparquet.write(path, df, file_scheme='simple', append=True)\n",
    "        zfs.snapshot_recursively('tss', pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = TimeSeriesStoreSingleThread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9600/9600 [00:50<00:00, 190.32it/s]\n"
     ]
    }
   ],
   "source": [
    "store.write([v[0] for v in valueses], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:40<00:00, 40.82s/it]\n"
     ]
    }
   ],
   "source": [
    "_ = [store.append([v[i] for v in valueses], i) for i in tqdm.tqdm(range(1, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = [store.append([v[i] for v in valueses], i) for i in tqdm.tqdm(range(2, 5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9600/9600 [00:17<00:00, 556.37it/s]\n",
      "100%|██████████| 9600/9600 [00:10<00:00, 874.68it/s]\n",
      "100%|██████████| 9600/9600 [00:38<00:00, 246.48it/s]\n"
     ]
    }
   ],
   "source": [
    "read_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in xrange(0, len(seq), size))\n",
    "\n",
    "class TimeSeriesStoreMultithread(object):\n",
    "    def __init__(self):\n",
    "        self.max_row_groups = 5\n",
    "        self.batch_size = 1000\n",
    "        \n",
    "    def __write__(self, idx_dfs):\n",
    "        for df_i, df in idx_dfs:\n",
    "            fastparquet.write('/tss/{}.parquet'.format(df_i), df, file_scheme='simple')\n",
    "    \n",
    "    def __append__(self, idx_dfs):\n",
    "        for df_i, df in idx_dfs:\n",
    "            path = '/tss/{}.parquet'.format(df_i)\n",
    "            if len(fastparquet.api.ParquetFile(path).row_groups) > self.max_row_groups:\n",
    "                fastparquet.write(path, fastparquet.api.ParquetFile(path).to_pandas(), file_scheme='simple')\n",
    "            fastparquet.write(path, df, file_scheme='simple', append=True)\n",
    "\n",
    "    def write(self, dfs, pid):\n",
    "        dfs_ids = zip(range(len(dfs)), dfs)\n",
    "        chunks = chunker(dfs_ids, self.batch_size)\n",
    "        ts = [Thread(target=self.__write__, args=[chunk]) for chunk in chunks]\n",
    "        _ = [t.start() for t in ts]\n",
    "        _ = [t.join() for t in ts]\n",
    "        zfs.snapshot_recursively('tss', pid)\n",
    "\n",
    "    def append(self, dfs, pid):\n",
    "        dfs_ids = zip(range(len(dfs)), dfs)\n",
    "        chunks = chunker(dfs_ids, self.batch_size)\n",
    "        ts = [Thread(target=self.__append__, args=[chunk]) for chunk in chunks]\n",
    "        _ = [t.start() for t in ts]\n",
    "        _ = [t.join() for t in ts]\n",
    "        zfs.snapshot_recursively('tss', pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = TimeSeriesStoreMultithread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 1min 41s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "store.write([v[0] for v in valueses], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:33<00:00, 93.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 1min 33s per loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "_ = [store.append([v[i] for v in valueses], i) for i in tqdm.tqdm(range(1, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9600/9600 [00:24<00:00, 384.25it/s]\n",
      "100%|██████████| 9600/9600 [00:10<00:00, 955.37it/s]\n",
      "100%|██████████| 9600/9600 [00:38<00:00, 248.48it/s]\n"
     ]
    }
   ],
   "source": [
    "read_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.session.SparkSession.Builder() \\\n",
    "    .config('spark.executor.memory', '16g') \\\n",
    "    .config('spark.driver.memory', '16g') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __write__(df, df_i):\n",
    "    fastparquet.write('/tss/{}.parquet'.format(df_i), df, file_scheme='simple')\n",
    "    return True\n",
    "    \n",
    "def __append__(idx_dfs):\n",
    "    raise NotIplementedError()\n",
    "        \n",
    "class TimeSeriesStoreSpark(object):\n",
    "    \n",
    "    def write(self, dfs, pid):\n",
    "        r = rdd.zipWithIndex().map(lambda (df, df_i): __write__(df, df_i)).collect()\n",
    "        zfs.snapshot_recursively('tss', pid)\n",
    "        return r\n",
    "\n",
    "    def append(self, dfs, pid):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = TimeSeriesStoreSpark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forex_loc = '/mnt/sdb/forex'\n",
    "paths = [os.path.join(forex_loc, f) for f in os.listdir(forex_loc)]\n",
    "\n",
    "valueses = sc.parallelize(paths, len(paths)) \\\n",
    "    .map(lambda p: pd.read_csv(p, names=['symbol', 'seriesdate', 'low', 'high'])) \\\n",
    "    .flatMap(lambda df: np.array_split(df, 250)) \\\n",
    "    .map(lambda x: split_df(x, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = valueses.map(lambda dfs dfs[0]).cache()\n",
    "_ = dfs.count()     # cache results\n",
    "store.write(dfs, 0) # 2 minutes ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TimeSeriesBatched(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.max_row_groups = 100\n",
    "    \n",
    "    def write(self, big_df, pid):\n",
    "        fastparquet.write('/tss/df.parquet', big_df, file_scheme='simple', partition_on=['symbol'])\n",
    "        zfs.snapshot_recursively('tss', pid)\n",
    "\n",
    "    def append(self, big_df, pid):\n",
    "        path = '/tss/df.parquet'\n",
    "        if len(fastparquet.api.ParquetFile(path).row_groups) > self.max_row_groups:\n",
    "            fastparquet.write(path, fastparquet.api.ParquetFile(path).to_pandas(), file_scheme='simple')\n",
    "        fastparquet.write(path, big_df, file_scheme='simple', partition_on=['symbol'], append=True)\n",
    "        zfs.snapshot_recursively('tss', pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = TimeSeriesBatched()\n",
    "big_df = pd.concat([v[0] for v in valueses])\n",
    "first_append = pd.concat([v[1] for v in valueses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 26.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "store.write(big_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 347 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "store.append(first_append, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [03:55<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    for i in tqdm.tqdm(xrange(2, n)):\n",
    "        df = pd.concat([v[i] for v in valueses])\n",
    "        store.append(first_append, i)\n",
    "        del df\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     USED  AVAIL  REFER  MOUNTPOINT\n",
      "tss@0   41.5K      -   748M  -\n",
      "tss@1   97.5K      -   748M  -\n",
      "tss@2    116K      -   749M  -\n",
      "tss@3     36K      -   749M  -\n",
      "tss@4     55K      -   749M  -\n",
      "tss@5   72.5K      -   750M  -\n",
      "tss@6   90.5K      -   750M  -\n",
      "tss@7    109K      -   750M  -\n",
      "tss@8     28K      -   750M  -\n",
      "tss@9     47K      -   751M  -\n",
      "tss@10  64.5K      -   751M  -\n",
      "tss@11    82K      -   751M  -\n",
      "tss@12   101K      -   752M  -\n",
      "tss@13   118K      -   752M  -\n",
      "tss@14  39.5K      -   752M  -\n",
      "tss@15  57.5K      -   753M  -\n",
      "tss@16    76K      -   753M  -\n",
      "tss@17    82K      -   753M  -\n",
      "tss@18    99K      -   754M  -\n",
      "tss@19   118K      -   754M  -\n",
      "tss@20    38K      -   754M  -\n",
      "tss@21  55.5K      -   754M  -\n",
      "tss@22  73.5K      -   755M  -\n",
      "tss@23  91.5K      -   755M  -\n",
      "tss@24   110K      -   755M  -\n",
      "tss@25  30.5K      -   756M  -\n",
      "tss@26  48.5K      -   756M  -\n",
      "tss@27  66.5K      -   756M  -\n",
      "tss@28  84.5K      -   757M  -\n",
      "tss@29   102K      -   757M  -\n",
      "tss@30   122K      -   757M  -\n",
      "tss@31  41.5K      -   757M  -\n",
      "tss@32    59K      -   758M  -\n",
      "tss@33    76K      -   758M  -\n",
      "tss@34  95.5K      -   758M  -\n",
      "tss@35   113K      -   759M  -\n",
      "tss@36  33.5K      -   759M  -\n",
      "tss@37    51K      -   759M  -\n",
      "tss@38    69K      -   760M  -\n",
      "tss@39  87.5K      -   760M  -\n",
      "tss@40   106K      -   760M  -\n",
      "tss@41   125K      -   761M  -\n",
      "tss@42    44K      -   761M  -\n",
      "tss@43  61.5K      -   761M  -\n",
      "tss@44    80K      -   761M  -\n",
      "tss@45    98K      -   762M  -\n",
      "tss@46   118K      -   762M  -\n",
      "tss@47    36K      -   762M  -\n",
      "tss@48  41.5K      -   763M  -\n",
      "tss@49  59.5K      -   763M  -\n",
      "tss@50    78K      -   763M  -\n",
      "tss@51    96K      -   764M  -\n",
      "tss@52   116K      -   764M  -\n",
      "tss@53  34.5K      -   764M  -\n",
      "tss@54  52.5K      -   765M  -\n",
      "tss@55    71K      -   765M  -\n",
      "tss@56  88.5K      -   765M  -\n",
      "tss@57   108K      -   765M  -\n",
      "tss@58   126K      -   766M  -\n",
      "tss@59  44.5K      -   766M  -\n",
      "tss@60    63K      -   766M  -\n",
      "tss@61  80.5K      -   767M  -\n",
      "tss@62  99.5K      -   767M  -\n",
      "tss@63   118K      -   767M  -\n",
      "tss@64    38K      -   768M  -\n",
      "tss@65  55.5K      -   768M  -\n",
      "tss@66  73.5K      -   768M  -\n",
      "tss@67    92K      -   769M  -\n",
      "tss@68   110K      -   769M  -\n",
      "tss@69   130K      -   769M  -\n",
      "tss@70    48K      -   769M  -\n",
      "tss@71  65.5K      -   770M  -\n",
      "tss@72  83.5K      -   770M  -\n",
      "tss@73   102K      -   770M  -\n",
      "tss@74   122K      -   771M  -\n",
      "tss@75  40.5K      -   771M  -\n",
      "tss@76    59K      -   771M  -\n",
      "tss@77    77K      -   772M  -\n",
      "tss@78  94.5K      -   772M  -\n",
      "tss@79   102K      -   772M  -\n",
      "tss@80   121K      -   772M  -\n",
      "tss@81  39.5K      -   773M  -\n",
      "tss@82  57.5K      -   773M  -\n",
      "tss@83    76K      -   773M  -\n",
      "tss@84    94K      -   774M  -\n",
      "tss@85   114K      -   774M  -\n",
      "tss@86    32K      -   774M  -\n",
      "tss@87  49.5K      -   775M  -\n",
      "tss@88  68.5K      -   775M  -\n",
      "tss@89    86K      -   775M  -\n",
      "tss@90   106K      -   776M  -\n",
      "tss@91   124K      -   776M  -\n",
      "tss@92    42K      -   776M  -\n",
      "tss@93    61K      -   776M  -\n",
      "tss@94    79K      -   777M  -\n",
      "tss@95  97.5K      -   777M  -\n",
      "tss@96   116K      -   777M  -\n",
      "tss@97    35K      -   778M  -\n",
      "tss@98  53.5K      -   778M  -\n",
      "tss@99      0      -   778M  -\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "zfs list -t snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_test():\n",
    "    path = '/tss/.zfs/snapshot/1/df.parquet'\n",
    "    \n",
    "    def full_series():\n",
    "        for p in tqdm.tqdm(paths):\n",
    "            df = pq.read_table(p).to_pandas()\n",
    "    \n",
    "    def last_value():\n",
    "        for p in tqdm.tqdm(paths):\n",
    "            pq_file = pq.ParquetFile(p)\n",
    "            last = pq_file.read_row_group(pq_file.num_row_groups - 1).to_pandas()[-1:]\n",
    "            \n",
    "    def all_values_one_day(day):\n",
    "        for p in tqdm.tqdm(paths):\n",
    "            df = pq.read_table(p).to_pandas()\n",
    "            df_one_day = df[df.seriesdate.str.startswith(day)]\n",
    "            \n",
    "    full_series()\n",
    "    last_value()\n",
    "    all_values_one_day('20090701')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/tss/.zfs/snapshot/1/df.parquet'\n",
    "pq_file = pq.ParquetFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 5.99 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "tmp = pq.read_pandas(path).to_pandas()\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/tss/df2.parquet'\n",
    "\n",
    "ds = pq.ParquetDataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file = fastparquet.ParquetFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq_file.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 20 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "fastparquet.write('/tss/df2.parquet', big_df, file_scheme='simple', partition_on=['symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyarrow.parquet' from '/home/mikeokslonger/anaconda2/lib/python2.7/site-packages/pyarrow/parquet.pyc'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
