{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from zfstools import connection\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastparquet\n",
    "import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2G\t/mnt/sdb/forex\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "du -sh /mnt/sdb/forex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/tss/spark': Device or resource busy\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "zfs list -t snapshot -o name | tail -n +2 | xargs -I {} zfs destroy {}\n",
    "rm -rf /tss/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_test():\n",
    "    paths = [os.path.join('/tss/.zfs/snapshot/1', f) for f in os.listdir('/tss/.zfs/snapshot/1')]\n",
    "    \n",
    "    def full_series():\n",
    "        for p in tqdm.tqdm(paths):\n",
    "            df = pq.read_table(p).to_pandas()\n",
    "    \n",
    "    def last_value():\n",
    "        for p in tqdm.tqdm(paths):\n",
    "            pq_file = pq.ParquetFile(p)\n",
    "            last = pq_file.read_row_group(pq_file.num_row_groups - 1).to_pandas()[-1:]\n",
    "            \n",
    "    def all_values_one_day(day):\n",
    "        for p in tqdm.tqdm(paths):\n",
    "            df = pq.read_table(p).to_pandas()\n",
    "            df_one_day = df[df.seriesdate.str.startswith(day)]\n",
    "            \n",
    "    full_series()\n",
    "    last_value()\n",
    "    all_values_one_day('20090701')\n",
    "    \n",
    "# Splits a df into n+1 parts where the first part consist of the first tot-n rows\n",
    "def split_df(df, n):\n",
    "    return [df[:-n]] + [df[n+i:n+i+1] for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forex_loc = '/mnt/sdb/forex'\n",
    "paths = [os.path.join(forex_loc, f) for f in os.listdir(forex_loc)]\n",
    "zfs = connection.ZFSConnection()\n",
    "names = ['symbol', 'seriesdate', 'low', 'high']\n",
    "\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10436374"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [df for p in paths[:5] for df in np.array_split(pd.read_csv(p, names=names), 400)]\n",
    "for i, df in zip(range(len(dfs)), dfs): df.symbol = i\n",
    "valueses = map(lambda x: split_df(x, n), dfs)\n",
    "sum(map(lambda x: len(x), dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TimeSeriesStoreSingleThread(object):\n",
    "    def __init__(self):\n",
    "        self.max_row_groups = 50\n",
    "\n",
    "    def write(self, dfs, pid):\n",
    "        for df_i, df in tqdm.tqdm(zip(range(len(dfs)), dfs)):\n",
    "            fastparquet.write('/tss/{}.parquet'.format(df_i), df, file_scheme='simple')\n",
    "        zfs.snapshot_recursively('tss', pid)\n",
    "    \n",
    "    def append(self, dfs, pid):\n",
    "        for df_i, df in zip(range(len(dfs)), dfs):\n",
    "            path = '/tss/{}.parquet'.format(df_i)\n",
    "            if len(fastparquet.api.ParquetFile(path).row_groups) > self.max_row_groups:\n",
    "                fastparquet.write(path, fastparquet.api.ParquetFile(path).to_pandas(), file_scheme='simple')\n",
    "            fastparquet.write(path, df, file_scheme='simple', append=True)\n",
    "        zfs.snapshot_recursively('tss', pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = TimeSeriesStoreSingleThread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:09<00:00, 200.04it/s]\n"
     ]
    }
   ],
   "source": [
    "store.write([v[0] for v in valueses], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.77s/it]\n"
     ]
    }
   ],
   "source": [
    "_ = [store.append([v[i] for v in valueses], i) for i in tqdm.tqdm(range(1, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:24<00:00,  8.21s/it]\n"
     ]
    }
   ],
   "source": [
    "_ = [store.append([v[i] for v in valueses], i) for i in tqdm.tqdm(range(2, 5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:07<00:00, 283.57it/s]\n",
      "100%|██████████| 2000/2000 [00:02<00:00, 995.86it/s] \n",
      "100%|██████████| 2000/2000 [00:07<00:00, 274.32it/s]\n"
     ]
    }
   ],
   "source": [
    "read_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in xrange(0, len(seq), size))\n",
    "\n",
    "class TimeSeriesStoreMultithread(object):\n",
    "    def __init__(self):\n",
    "        self.max_row_groups = 5\n",
    "        self.batch_size = 1000\n",
    "        \n",
    "    def __write__(self, idx_dfs):\n",
    "        for df_i, df in idx_dfs:\n",
    "            fastparquet.write('/tss/{}.parquet'.format(df_i), df, file_scheme='simple')\n",
    "    \n",
    "    def __append__(self, idx_dfs):\n",
    "        for df_i, df in idx_dfs:\n",
    "            path = '/tss/{}.parquet'.format(df_i)\n",
    "            if len(fastparquet.api.ParquetFile(path).row_groups) > self.max_row_groups:\n",
    "                fastparquet.write(path, fastparquet.api.ParquetFile(path).to_pandas(), file_scheme='simple')\n",
    "            fastparquet.write(path, df, file_scheme='simple', append=True)\n",
    "\n",
    "    def write(self, dfs, pid):\n",
    "        dfs_ids = zip(range(len(dfs)), dfs)\n",
    "        chunks = chunker(dfs_ids, self.batch_size)\n",
    "        ts = [Thread(target=self.__write__, args=[chunk]) for chunk in chunks]\n",
    "        _ = [t.start() for t in ts]\n",
    "        _ = [t.join() for t in ts]\n",
    "        zfs.snapshot_recursively('tss', pid)\n",
    "\n",
    "    def append(self, dfs, pid):\n",
    "        dfs_ids = zip(range(len(dfs)), dfs)\n",
    "        chunks = chunker(dfs_ids, self.batch_size)\n",
    "        ts = [Thread(target=self.__append__, args=[chunk]) for chunk in chunks]\n",
    "        _ = [t.start() for t in ts]\n",
    "        _ = [t.join() for t in ts]\n",
    "        zfs.snapshot_recursively('tss', pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = TimeSeriesStoreMultithread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 16.7 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "store.write([v[0] for v in valueses], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 16.2 s per loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "_ = [store.append([v[i] for v in valueses], i) for i in tqdm.tqdm(range(1, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:05<00:00, 399.09it/s]\n",
      "100%|██████████| 2000/2000 [00:02<00:00, 913.70it/s]\n",
      "100%|██████████| 2000/2000 [00:07<00:00, 253.50it/s]\n"
     ]
    }
   ],
   "source": [
    "read_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.session.SparkSession.Builder() \\\n",
    "    .config('spark.executor.memory', '16g') \\\n",
    "    .config('spark.driver.memory', '16g') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TimeSeriesBatched(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.max_row_groups = 100\n",
    "    \n",
    "    def write(self, big_df, pid):\n",
    "        fastparquet.write('/tss/df.parquet', big_df, file_scheme='simple', partition_on=['symbol'])\n",
    "        zfs.snapshot_recursively('tss', pid)\n",
    "\n",
    "    def append(self, big_df, pid):\n",
    "        path = '/tss/df.parquet'\n",
    "        if len(fastparquet.api.ParquetFile(path).row_groups) > self.max_row_groups:\n",
    "            fastparquet.write(path, fastparquet.api.ParquetFile(path).to_pandas(), file_scheme='simple')\n",
    "        fastparquet.write(path, big_df, file_scheme='simple', partition_on=['symbol'], append=True)\n",
    "        zfs.snapshot_recursively('tss', pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = TimeSeriesBatched()\n",
    "big_df = pd.concat([v[0] for v in valueses])\n",
    "first_append = pd.concat([v[1] for v in valueses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 8.75 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "store.write(big_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 78.5 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "store.append(first_append, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:46<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    for i in tqdm.tqdm(xrange(2, n)):\n",
    "        df = pd.concat([v[i] for v in valueses])\n",
    "        store.append(first_append, i)\n",
    "        del df\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     USED  AVAIL  REFER  MOUNTPOINT\n",
      "tss@0     45K      -   154M  -\n",
      "tss@1     60K      -   154M  -\n",
      "tss@2   52.5K      -   154M  -\n",
      "tss@3     42K      -   154M  -\n",
      "tss@4   29.5K      -   154M  -\n",
      "tss@5     94K      -   154M  -\n",
      "tss@6   84.5K      -   154M  -\n",
      "tss@7   75.5K      -   154M  -\n",
      "tss@8   67.5K      -   154M  -\n",
      "tss@9   61.5K      -   154M  -\n",
      "tss@10    54K      -   154M  -\n",
      "tss@11    43K      -   154M  -\n",
      "tss@12    31K      -   154M  -\n",
      "tss@13  95.5K      -   154M  -\n",
      "tss@14    86K      -   154M  -\n",
      "tss@15    77K      -   155M  -\n",
      "tss@16    69K      -   155M  -\n",
      "tss@17    62K      -   155M  -\n",
      "tss@18  54.5K      -   155M  -\n",
      "tss@19  44.5K      -   155M  -\n",
      "tss@20  32.5K      -   155M  -\n",
      "tss@21    97K      -   155M  -\n",
      "tss@22  87.5K      -   155M  -\n",
      "tss@23  78.5K      -   155M  -\n",
      "tss@24  70.5K      -   155M  -\n",
      "tss@25  63.5K      -   155M  -\n",
      "tss@26    56K      -   155M  -\n",
      "tss@27    45K      -   155M  -\n",
      "tss@28    33K      -   155M  -\n",
      "tss@29  98.5K      -   155M  -\n",
      "tss@30  87.5K      -   156M  -\n",
      "tss@31  78.5K      -   156M  -\n",
      "tss@32    72K      -   156M  -\n",
      "tss@33    53K      -   156M  -\n",
      "tss@34  45.5K      -   156M  -\n",
      "tss@35  34.5K      -   156M  -\n",
      "tss@36  22.5K      -   156M  -\n",
      "tss@37    88K      -   156M  -\n",
      "tss@38    77K      -   156M  -\n",
      "tss@39    68K      -   156M  -\n",
      "tss@40  60.5K      -   156M  -\n",
      "tss@41  54.5K      -   156M  -\n",
      "tss@42  46.5K      -   156M  -\n",
      "tss@43    36K      -   156M  -\n",
      "tss@44  23.5K      -   156M  -\n",
      "tss@45    89K      -   156M  -\n",
      "tss@46  78.5K      -   157M  -\n",
      "tss@47  69.5K      -   157M  -\n",
      "tss@48    62K      -   157M  -\n",
      "tss@49  54.5K      -   157M  -\n",
      "tss@50    47K      -   157M  -\n",
      "tss@51    36K      -   157M  -\n",
      "tss@52    24K      -   157M  -\n",
      "tss@53    91K      -   157M  -\n",
      "tss@54    80K      -   157M  -\n",
      "tss@55    71K      -   157M  -\n",
      "tss@56  63.5K      -   157M  -\n",
      "tss@57    56K      -   157M  -\n",
      "tss@58  48.5K      -   157M  -\n",
      "tss@59  37.5K      -   157M  -\n",
      "tss@60  25.5K      -   157M  -\n",
      "tss@61    91K      -   158M  -\n",
      "tss@62    80K      -   158M  -\n",
      "tss@63  71.5K      -   158M  -\n",
      "tss@64    64K      -   158M  -\n",
      "tss@65  57.5K      -   158M  -\n",
      "tss@66    50K      -   158M  -\n",
      "tss@67    39K      -   158M  -\n",
      "tss@68    27K      -   158M  -\n",
      "tss@69  92.5K      -   158M  -\n",
      "tss@70  81.5K      -   158M  -\n",
      "tss@71    73K      -   158M  -\n",
      "tss@72  65.5K      -   158M  -\n",
      "tss@73    58K      -   158M  -\n",
      "tss@74  50.5K      -   158M  -\n",
      "tss@75  39.5K      -   158M  -\n",
      "tss@76    28K      -   158M  -\n",
      "tss@77  93.5K      -   159M  -\n",
      "tss@78    84K      -   159M  -\n",
      "tss@79  74.5K      -   159M  -\n",
      "tss@80    67K      -   159M  -\n",
      "tss@81  59.5K      -   159M  -\n",
      "tss@82    52K      -   159M  -\n",
      "tss@83  40.5K      -   159M  -\n",
      "tss@84   106K      -   159M  -\n",
      "tss@85    94K      -   159M  -\n",
      "tss@86  83.5K      -   159M  -\n",
      "tss@87    75K      -   159M  -\n",
      "tss@88  68.5K      -   159M  -\n",
      "tss@89    61K      -   159M  -\n",
      "tss@90  53.5K      -   159M  -\n",
      "tss@91    42K      -   159M  -\n",
      "tss@92   108K      -   159M  -\n",
      "tss@93  95.5K      -   160M  -\n",
      "tss@94  85.5K      -   160M  -\n",
      "tss@95  76.5K      -   160M  -\n",
      "tss@96    69K      -   160M  -\n",
      "tss@97  61.5K      -   160M  -\n",
      "tss@98  53.5K      -   160M  -\n",
      "tss@99      0      -   160M  -\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "zfs list -t snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 2.78 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "path = '/tss/.zfs/snapshot/1/df.parquet'\n",
    "tmp = pq.read_pandas(path).to_pandas()\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesStoreHive(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.max_row_groups = 100\n",
    "    \n",
    "    def write(self, big_df, pid):\n",
    "        fastparquet.write('/tss/df.parquet', big_df, file_scheme='hive', partition_on=['symbol'])\n",
    "        zfs.snapshot_recursively('tss', pid)\n",
    "\n",
    "    def append(self, big_df, pid):\n",
    "        path = '/tss/df.parquet'\n",
    "        fastparquet.write(path, big_df, file_scheme='hive', partition_on=['symbol'], append=True)\n",
    "        zfs.snapshot_recursively('tss', pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = TimeSeriesStoreHive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 8.63 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "store.write(big_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 4.72 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "store.append(first_append, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 130 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "tmp = spark.read.parquet('/tss/df.parquet/symbol=1').toPandas()\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesStoreSpark(object):\n",
    "    \n",
    "    def write(self, df, pid):\n",
    "        df.write.mode('overwrite').partitionBy('symbol').parquet('/tss/spark/df.parquet')\n",
    "        zfs.snapshot_recursively('tss/spark', pid)\n",
    "\n",
    "    def append(self, big_df, pid):\n",
    "        path = '/tss/spark/df.parquet'\n",
    "        spark_append.write.mode('append').partitionBy('symbol').parquet('/tss/spark/df.parquet')\n",
    "        zfs.snapshot_recursively('tss/spark', pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = TimeSeriesStoreSpark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_df = spark.read.parquet('/tss/df.parquet').cache()\n",
    "_ = spark_df.count()\n",
    "fastparquet.write('/tss/an_append.parquet', first_append, file_scheme='hive', partition_on=['symbol'])\n",
    "spark_append = spark.read.parquet('/tss/an_append.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 9.13 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "store.write(spark_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236374\n",
      "1 loop, best of 1: 3.03 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "print spark.read.parquet('/tss/spark/df.parquet').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 1: 4.17 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "store.append(spark_append, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4742\n",
      "1 loop, best of 1: 79.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "print spark.read.parquet('/tss/spark/df.parquet/symbol=1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
